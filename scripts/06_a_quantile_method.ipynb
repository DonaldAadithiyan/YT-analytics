{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89792010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68228b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = pd.read_csv(\"../data/pre-processed/preprocessed_youtube_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8f9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime if not already\n",
    "preprocessed_dataset['published_time'] = pd.to_datetime(preprocessed_dataset['published_time'], format='%H:%M:%S', errors='coerce')\n",
    "\n",
    "# Extract hour and minute\n",
    "preprocessed_dataset['published_hour'] = preprocessed_dataset['published_time'].dt.hour\n",
    "preprocessed_dataset['published_minute'] = preprocessed_dataset['published_time'].dt.minute\n",
    "\n",
    "# Drop the original time column\n",
    "preprocessed_dataset.drop(columns=['published_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ccedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate label encoders for each column\n",
    "le_day = LabelEncoder()\n",
    "le_def = LabelEncoder()\n",
    "\n",
    "preprocessed_dataset['published_day_of_week'] = le_day.fit_transform(preprocessed_dataset['published_day_of_week'])\n",
    "preprocessed_dataset['definition'] = le_def.fit_transform(preprocessed_dataset['definition'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657b3c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with all four columns NaN: 12\n",
      "       category_id country  definition logged_at_initial  view_count_initial  \\\n",
      "13893          NaN     NaN           2               NaN                 NaN   \n",
      "14898          NaN     NaN           2               NaN                 NaN   \n",
      "18678          NaN     NaN           2               NaN                 NaN   \n",
      "22052          NaN     NaN           2               NaN                 NaN   \n",
      "28135          NaN     NaN           2               NaN                 NaN   \n",
      "29196          NaN     NaN           2               NaN                 NaN   \n",
      "31200          NaN     NaN           2               NaN                 NaN   \n",
      "32603          NaN     NaN           2               NaN                 NaN   \n",
      "38926          NaN     NaN           2               NaN                 NaN   \n",
      "41143          NaN     NaN           2               NaN                 NaN   \n",
      "43640          NaN     NaN           2               NaN                 NaN   \n",
      "45789          NaN     NaN           2               NaN                 NaN   \n",
      "\n",
      "       like_count_initial  c_view_count_initial logged_at_final  \\\n",
      "13893                 NaN                   NaN             NaN   \n",
      "14898                 NaN                   NaN             NaN   \n",
      "18678                 NaN                   NaN             NaN   \n",
      "22052                 NaN                   NaN             NaN   \n",
      "28135                 NaN                   NaN             NaN   \n",
      "29196                 NaN                   NaN             NaN   \n",
      "31200                 NaN                   NaN             NaN   \n",
      "32603                 NaN                   NaN             NaN   \n",
      "38926                 NaN                   NaN             NaN   \n",
      "41143                 NaN                   NaN             NaN   \n",
      "43640                 NaN                   NaN             NaN   \n",
      "45789                 NaN                   NaN             NaN   \n",
      "\n",
      "       view_count_final  like_count_final  ...  channel_avg_views  \\\n",
      "13893               NaN               NaN  ...                NaN   \n",
      "14898               NaN               NaN  ...                NaN   \n",
      "18678               NaN               NaN  ...                NaN   \n",
      "22052               NaN               NaN  ...                NaN   \n",
      "28135               NaN               NaN  ...                NaN   \n",
      "29196               NaN               NaN  ...                NaN   \n",
      "31200               NaN               NaN  ...                NaN   \n",
      "32603               NaN               NaN  ...                NaN   \n",
      "38926               NaN               NaN  ...                NaN   \n",
      "41143               NaN               NaN  ...                NaN   \n",
      "43640               NaN               NaN  ...                NaN   \n",
      "45789               NaN               NaN  ...                NaN   \n",
      "\n",
      "       channel_growth_potential  channel_virality_score  log_channel_subs  \\\n",
      "13893                       NaN                     NaN               NaN   \n",
      "14898                       NaN                     NaN               NaN   \n",
      "18678                       NaN                     NaN               NaN   \n",
      "22052                       NaN                     NaN               NaN   \n",
      "28135                       NaN                     NaN               NaN   \n",
      "29196                       NaN                     NaN               NaN   \n",
      "31200                       NaN                     NaN               NaN   \n",
      "32603                       NaN                     NaN               NaN   \n",
      "38926                       NaN                     NaN               NaN   \n",
      "41143                       NaN                     NaN               NaN   \n",
      "43640                       NaN                     NaN               NaN   \n",
      "45789                       NaN                     NaN               NaN   \n",
      "\n",
      "       relative_views_to_category  relative_likes_to_category  \\\n",
      "13893                         NaN                         NaN   \n",
      "14898                         NaN                         NaN   \n",
      "18678                         NaN                         NaN   \n",
      "22052                         NaN                         NaN   \n",
      "28135                         NaN                         NaN   \n",
      "29196                         NaN                         NaN   \n",
      "31200                         NaN                         NaN   \n",
      "32603                         NaN                         NaN   \n",
      "38926                         NaN                         NaN   \n",
      "41143                         NaN                         NaN   \n",
      "43640                         NaN                         NaN   \n",
      "45789                         NaN                         NaN   \n",
      "\n",
      "       logged_at_initial_hour  logged_at_final_hour  published_hour  \\\n",
      "13893                     NaN                   NaN              11   \n",
      "14898                     NaN                   NaN               5   \n",
      "18678                     NaN                   NaN              22   \n",
      "22052                     NaN                   NaN               9   \n",
      "28135                     NaN                   NaN               6   \n",
      "29196                     NaN                   NaN               9   \n",
      "31200                     NaN                   NaN              22   \n",
      "32603                     NaN                   NaN              22   \n",
      "38926                     NaN                   NaN               8   \n",
      "41143                     NaN                   NaN              22   \n",
      "43640                     NaN                   NaN               8   \n",
      "45789                     NaN                   NaN              22   \n",
      "\n",
      "       published_minute  \n",
      "13893                55  \n",
      "14898                59  \n",
      "18678                51  \n",
      "22052                44  \n",
      "28135                58  \n",
      "29196                12  \n",
      "31200                 5  \n",
      "32603                52  \n",
      "38926                59  \n",
      "41143                51  \n",
      "43640                51  \n",
      "45789                55  \n",
      "\n",
      "[12 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "missing_mask = preprocessed_dataset[['like_count_initial', 'like_count_final', 'view_count_initial', 'view_count_final']].isnull()\n",
    "\n",
    "# Check rows where all four columns are NaN\n",
    "rows_all_nan = missing_mask.all(axis=1)\n",
    "\n",
    "print(\"Number of rows with all four columns NaN:\", rows_all_nan.sum())\n",
    "\n",
    "# Optionally, see those rows\n",
    "print(preprocessed_dataset[rows_all_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b04c907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with any of the four columns NaN: 682\n"
     ]
    }
   ],
   "source": [
    "rows_any_nan = missing_mask.any(axis=1)\n",
    "print(\"Rows with any of the four columns NaN:\", rows_any_nan.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b35af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with any of the four columns NaN: 682\n"
     ]
    }
   ],
   "source": [
    "rows_any_nan = missing_mask.any(axis=1)\n",
    "print(\"Rows with any of the four columns NaN:\", rows_any_nan.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2402f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert to category\n",
    "preprocessed_dataset['country'] = preprocessed_dataset['country'].astype('category')\n",
    "\n",
    "# Step 2: Save mapping BEFORE encoding\n",
    "categories = list(preprocessed_dataset['country'].cat.categories)\n",
    "country_to_code = {country: idx for idx, country in enumerate(categories)}\n",
    "\n",
    "with open(\"country_encoding.json\", \"w\") as f:\n",
    "    json.dump(country_to_code, f)\n",
    "\n",
    "# Step 3: Encode with .cat.codes\n",
    "preprocessed_dataset['country_encoded'] = preprocessed_dataset['country'].cat.codes\n",
    "\n",
    "\n",
    "preprocessed_dataset['category_id'] = (\n",
    "    preprocessed_dataset['category_id']\n",
    "    .fillna(-1)  # or any placeholder like 999\n",
    "    .astype(int)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d824297",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset.drop(columns=['logged_at_final', 'logged_at_initial','country'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5675be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = preprocessed_dataset.dropna(subset=[\n",
    "    'like_count_initial',\n",
    "    'like_count_final',\n",
    "    'view_count_initial',\n",
    "    'view_count_final'\n",
    "])\n",
    "\n",
    "# Define target columns\n",
    "target_columns = [\n",
    "    'like_count_initial',\n",
    "    'like_count_final',\n",
    "    'view_count_initial',\n",
    "    'view_count_final'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71584dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before outlier removal: 51862\n",
      "Rows after outlier removal: 51398\n"
     ]
    }
   ],
   "source": [
    "# Choose columns to check for outliers, e.g. target columns\n",
    "cols_to_check = target_columns  # or other numeric features\n",
    "\n",
    "z_scores = np.abs(stats.zscore(preprocessed_dataset[cols_to_check]))\n",
    "# Define threshold, e.g. 3 std deviations\n",
    "threshold = 3\n",
    "# Keep only rows where all z-scores are below threshold (no outlier)\n",
    "non_outliers = (z_scores < threshold).all(axis=1)\n",
    "\n",
    "\n",
    "print(f\"Rows before outlier removal: {len(preprocessed_dataset)}\")\n",
    "preprocessed_dataset = preprocessed_dataset[non_outliers]\n",
    "print(f\"Rows after outlier removal: {len(preprocessed_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e511304",
   "metadata": {},
   "outputs": [],
   "source": [
    "q25 = preprocessed_dataset['view_count_final'].quantile(0.25)\n",
    "q75 = preprocessed_dataset['view_count_final'].quantile(0.75)\n",
    "\n",
    "df_low = preprocessed_dataset[preprocessed_dataset['view_count_final'] <= q25]\n",
    "df_mid = preprocessed_dataset[(preprocessed_dataset['view_count_final'] > q25) & (preprocessed_dataset['view_count_final'] <= q75)]\n",
    "df_high = preprocessed_dataset[preprocessed_dataset['view_count_final'] > q75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67e03a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low quantile stats:\n",
      "                        mean  median  mode\n",
      "like_count_initial  0.744424     0.0   0.0\n",
      "like_count_final    0.894701     0.0   0.0\n",
      "view_count_initial  4.539665     0.0   0.0\n",
      "view_count_final    1.100359     0.0   0.0\n",
      "\n",
      "Mid quantile stats:\n",
      "                          mean  median  mode\n",
      "like_count_initial    6.130695     2.0   0.0\n",
      "like_count_final      9.744404     4.0   0.0\n",
      "view_count_initial  187.141888   116.0   0.0\n",
      "view_count_final    275.472813   230.0   8.0\n",
      "\n",
      "High quantile stats:\n",
      "                           mean  median   mode\n",
      "like_count_initial   120.337227    23.0    0.0\n",
      "like_count_final     225.422664    49.0    0.0\n",
      "view_count_initial  3045.790576  1067.0    0.0\n",
      "view_count_final    5922.651246  2058.5  873.0\n"
     ]
    }
   ],
   "source": [
    "def describe_stats(df, cols):\n",
    "    mean_vals = df[cols].mean()\n",
    "    median_vals = df[cols].median()\n",
    "    mode_vals = df[cols].mode().iloc[0]  # mode() returns DataFrame, take first mode row\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'mean': mean_vals,\n",
    "        'median': median_vals,\n",
    "        'mode': mode_vals\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "print(\"Low quantile stats:\")\n",
    "print(describe_stats(df_low, target_columns))\n",
    "\n",
    "print(\"\\nMid quantile stats:\")\n",
    "print(describe_stats(df_mid, target_columns))\n",
    "\n",
    "print(\"\\nHigh quantile stats:\")\n",
    "print(describe_stats(df_high, target_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1793630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13362, 23)\n",
      "(25196, 23)\n",
      "(12840, 23)\n"
     ]
    }
   ],
   "source": [
    "print(df_low.shape)\n",
    "print(df_mid.shape)\n",
    "print(df_high.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0569681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_low = df_low.drop(columns=target_columns)\n",
    "y_low = df_low[target_columns]\n",
    "\n",
    "X_mid = df_mid.drop(columns=target_columns)\n",
    "y_mid = df_mid[target_columns]\n",
    "\n",
    "X_high = df_high.drop(columns=target_columns)\n",
    "y_high = df_high[target_columns]\n",
    "\n",
    "X_low_train, X_low_valid, y_low_train, y_low_valid = train_test_split(\n",
    "    X_low, y_low, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_mid_train, X_mid_valid, y_mid_train, y_mid_valid = train_test_split(\n",
    "    X_mid, y_mid, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_high_train, X_high_valid, y_high_train, y_high_valid = train_test_split(\n",
    "    X_high, y_high, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1106cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_high_train_log = np.log1p(y_high_train)\n",
    "y_mid_train_log = np.log1p(y_mid_train)\n",
    "y_low_train_log = np.log1p(y_low_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41cb1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 20:39:10,303] A new study created in memory with name: XGBoost_Optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 20:39:18,340] Trial 0 finished with value: 1.4029244384411648e+16 and parameters: {'n_estimators': 1710, 'learning_rate': 0.09663647934733707, 'max_depth': 14, 'min_child_weight': 8, 'subsample': 0.8911038928322303, 'colsample_bytree': 0.990767441078574, 'gamma': 0.56111803675693, 'reg_alpha': 0.7383602996523383, 'reg_lambda': 0.09322553539764222}. Best is trial 0 with value: 1.4029244384411648e+16.\n",
      "[I 2025-08-08 20:39:24,531] Trial 1 finished with value: 1.2669946291027968e+16 and parameters: {'n_estimators': 1089, 'learning_rate': 0.12296026056864931, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.7111112351513763, 'colsample_bytree': 0.6861109751096797, 'gamma': 0.11969774023027546, 'reg_alpha': 0.23514175456406705, 'reg_lambda': 0.529858255177841}. Best is trial 1 with value: 1.2669946291027968e+16.\n",
      "[I 2025-08-08 20:39:35,845] Trial 2 finished with value: 1.5157177958268928e+16 and parameters: {'n_estimators': 630, 'learning_rate': 0.08149588616401418, 'max_depth': 11, 'min_child_weight': 3, 'subsample': 0.6672896640164919, 'colsample_bytree': 0.7773341394067994, 'gamma': 0.12127295393437876, 'reg_alpha': 0.985099168687221, 'reg_lambda': 0.7291172523474008}. Best is trial 1 with value: 1.2669946291027968e+16.\n",
      "[I 2025-08-08 20:39:46,426] Trial 3 finished with value: 1.4588638104911872e+16 and parameters: {'n_estimators': 672, 'learning_rate': 0.06355764983181565, 'max_depth': 11, 'min_child_weight': 5, 'subsample': 0.632229144813048, 'colsample_bytree': 0.657968527220273, 'gamma': 0.40912243543057203, 'reg_alpha': 0.0028063617892222226, 'reg_lambda': 0.1874526890656648}. Best is trial 1 with value: 1.2669946291027968e+16.\n",
      "[I 2025-08-08 20:39:55,181] Trial 4 finished with value: 1.4285765601132544e+16 and parameters: {'n_estimators': 2343, 'learning_rate': 0.09415008601832858, 'max_depth': 9, 'min_child_weight': 5, 'subsample': 0.9605929845462017, 'colsample_bytree': 0.774006223728607, 'gamma': 0.512856037661238, 'reg_alpha': 0.09776783933804345, 'reg_lambda': 0.2925937905469709}. Best is trial 1 with value: 1.2669946291027968e+16.\n",
      "[I 2025-08-08 20:40:02,537] Trial 5 finished with value: 1.6755713246232576e+16 and parameters: {'n_estimators': 1886, 'learning_rate': 0.19267036338997773, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.8002018854227592, 'colsample_bytree': 0.8717339760317641, 'gamma': 0.9807612390148871, 'reg_alpha': 0.12626661812149853, 'reg_lambda': 0.08316635770361447}. Best is trial 1 with value: 1.2669946291027968e+16.\n",
      "[I 2025-08-08 20:40:23,161] Trial 6 finished with value: 1.2668416208928768e+16 and parameters: {'n_estimators': 3314, 'learning_rate': 0.025746278531409112, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.9018818608280503, 'colsample_bytree': 0.7568780839066979, 'gamma': 0.550676890668629, 'reg_alpha': 0.33634325189798364, 'reg_lambda': 0.25745029524510865}. Best is trial 6 with value: 1.2668416208928768e+16.\n",
      "[I 2025-08-08 20:40:29,818] Trial 7 finished with value: 1.3252068978458624e+16 and parameters: {'n_estimators': 2329, 'learning_rate': 0.15908311848791523, 'max_depth': 7, 'min_child_weight': 6, 'subsample': 0.9654786467806038, 'colsample_bytree': 0.9096498549539532, 'gamma': 0.9386635973615182, 'reg_alpha': 0.8415471912760869, 'reg_lambda': 0.5297606009770442}. Best is trial 6 with value: 1.2668416208928768e+16.\n",
      "[I 2025-08-08 20:40:47,803] Trial 8 finished with value: 1.7343342081736704e+16 and parameters: {'n_estimators': 2543, 'learning_rate': 0.12365218165016355, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.7052300867828122, 'colsample_bytree': 0.7959211335499295, 'gamma': 0.6263044325243485, 'reg_alpha': 0.2865584258205017, 'reg_lambda': 0.6942843603816162}. Best is trial 6 with value: 1.2668416208928768e+16.\n",
      "[I 2025-08-08 20:41:03,332] Trial 9 finished with value: 1.849197954544435e+16 and parameters: {'n_estimators': 1820, 'learning_rate': 0.14841617220319195, 'max_depth': 8, 'min_child_weight': 9, 'subsample': 0.6375336053165301, 'colsample_bytree': 0.9240816669607496, 'gamma': 0.43253980902632505, 'reg_alpha': 0.38456607639386775, 'reg_lambda': 0.23165142187662535}. Best is trial 6 with value: 1.2668416208928768e+16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost Results ===\n",
      "Best parameters: {'n_estimators': 3314, 'learning_rate': 0.025746278531409112, 'max_depth': 10, 'min_child_weight': 2, 'subsample': 0.9018818608280503, 'colsample_bytree': 0.7568780839066979, 'gamma': 0.550676890668629, 'reg_alpha': 0.33634325189798364, 'reg_lambda': 0.25745029524510865}\n",
      "Best MAPE: 1.2668416208928768e+16\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed_dataset.drop(columns=target_columns)\n",
    "y = preprocessed_dataset[target_columns]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 4000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 14),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_log,\n",
    "        eval_set=[(X_valid, np.log1p(y_valid))],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds_log = model.predict(X_valid)\n",
    "    preds = np.expm1(preds_log)\n",
    "\n",
    "    mape = mean_absolute_percentage_error(y_valid, preds)\n",
    "    return mape\n",
    "\n",
    "# Run Optuna optimization\n",
    "xgb_study = optuna.create_study(direction=\"minimize\", study_name=\"XGBoost_Optimization\")\n",
    "xgb_study.optimize(xgb_objective, n_trials=10)\n",
    "\n",
    "print(\"=== XGBoost Results ===\")\n",
    "print(\"Best parameters:\", xgb_study.best_trial.params)\n",
    "print(\"Best MAPE:\", xgb_study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c3244d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(**xgb_study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24236781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAE: 524.650146484375\n",
      "\n",
      "MAE for each output column:\n",
      "like_count_initial: 28.7724\n",
      "like_count_final: 52.8419\n",
      "view_count_initial: 692.4897\n",
      "view_count_final: 1324.4967\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(**xgb_study.best_trial.params)\n",
    "\n",
    "xgb_model2 = MultiOutputRegressor(xgb_model)\n",
    "xgb_model2.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "preds_log = xgb_model2.predict(X_valid)\n",
    "preds = np.expm1(preds_log)\n",
    "\n",
    "overall_mae = mean_absolute_error(y_valid, preds)\n",
    "print(\"Overall MAE:\", overall_mae)\n",
    "\n",
    "print(\"\\nMAE for each output column:\")\n",
    "for col_idx in range(y_valid.shape[1]):\n",
    "    col_mae = mean_absolute_error(y_valid.iloc[:, col_idx], preds[:, col_idx])\n",
    "    print(f\"{target_columns[col_idx]}: {col_mae:.4f}\")\n",
    "\n",
    "# Optional: Manual MAPE check\n",
    "# manual_mape = np.mean(np.abs((y_valid.to_numpy() - preds) / y_valid.to_numpy())) * 100\n",
    "# print(\"Manual MAPE:\", manual_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30f88fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(xgb_model2, 'xgb_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
